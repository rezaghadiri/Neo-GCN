{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiDdqsH2vyy4",
        "outputId": "05637019-a694-45ab-9d7d-7abc863ccc52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch==2.0.0+cu118\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.0.0%2Bcu118-cp310-cp310-linux_x86_64.whl (2267.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0+cu118) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0+cu118) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0+cu118) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0+cu118) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0+cu118) (3.1.4)\n",
            "Collecting triton==2.0.0 (from torch==2.0.0+cu118)\n",
            "  Downloading https://download.pytorch.org/whl/triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0+cu118) (3.30.2)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.0+cu118)\n",
            "  Downloading https://download.pytorch.org/whl/lit-15.0.7.tar.gz (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.0+cu118) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.0+cu118) (1.3.0)\n",
            "Building wheels for collected packages: lit\n",
            "  Building wheel for lit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lit: filename=lit-15.0.7-py3-none-any.whl size=89990 sha256=9b2c79036629cb703a939448e9c86f9d424836811d6dce61ccd25ad18c255482\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/2c/b6/3ed2983b1b44fe0dea1bb35234b09f2c22fb8ebb308679c922\n",
            "Successfully built lit\n",
            "Installing collected packages: lit, triton, torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.4.0+cu121\n",
            "    Uninstalling torch-2.4.0+cu121:\n",
            "      Successfully uninstalled torch-2.4.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.4.0+cu121 requires torch==2.4.0, but you have torch 2.0.0+cu118 which is incompatible.\n",
            "torchvision 0.19.0+cu121 requires torch==2.4.0, but you have torch 2.0.0+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed lit-15.0.7 torch-2.0.0+cu118 triton-2.0.0\n",
            "Collecting torch_geometric==2.3.1\n",
            "  Downloading torch_geometric-2.3.1.tar.gz (661 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.3.1) (4.66.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.3.1) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.3.1) (1.13.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.3.1) (3.1.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.3.1) (2.32.3)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.3.1) (3.1.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.3.1) (1.3.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.3.1) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric==2.3.1) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric==2.3.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric==2.3.1) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric==2.3.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric==2.3.1) (2024.8.30)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric==2.3.1) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric==2.3.1) (3.5.0)\n",
            "Building wheels for collected packages: torch_geometric\n",
            "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_geometric: filename=torch_geometric-2.3.1-py3-none-any.whl size=910459 sha256=cfd63b10aed91ef53c8053381cd4a1e4fd9250b9f533c7f326a241df17f9530e\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/dc/30/e2874821ff308ee67dcd7a66dbde912411e19e35a1addda028\n",
            "Successfully built torch_geometric\n",
            "Installing collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.3.1\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.0.0+cu118.html\n",
            "Collecting pyg_lib\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/pyg_lib-0.4.0%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_scatter-2.1.2%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (10.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_sparse-0.6.18%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_cluster-1.6.3%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_spline_conv\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_spline_conv-1.2.2%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (886 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m886.6/886.6 kB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_sparse) (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy->torch_sparse) (1.26.4)\n",
            "Installing collected packages: torch_spline_conv, torch_scatter, pyg_lib, torch_sparse, torch_cluster\n",
            "Successfully installed pyg_lib-0.4.0+pt20cu118 torch_cluster-1.6.3+pt20cu118 torch_scatter-2.1.2+pt20cu118 torch_sparse-0.6.18+pt20cu118 torch_spline_conv-1.2.2+pt20cu118\n",
            "Collecting deepsnap\n",
            "  Downloading deepsnap-0.2.1.tar.gz (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.3/68.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from deepsnap) (2.0.0+cu118)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from deepsnap) (3.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deepsnap) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->deepsnap) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->deepsnap) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->deepsnap) (1.13.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->deepsnap) (3.1.4)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->deepsnap) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->deepsnap) (3.30.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->deepsnap) (15.0.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->deepsnap) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->deepsnap) (1.3.0)\n",
            "Building wheels for collected packages: deepsnap\n",
            "  Building wheel for deepsnap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepsnap: filename=deepsnap-0.2.1-py3-none-any.whl size=76109 sha256=5886d056186780c8343bfe10149d187d33ae76b02ced64e32f7b257a7d895361\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/74/c7/7232a4a305ccccd2b51d7006028eba0cd7c72927e0d60d73ba\n",
            "Successfully built deepsnap\n",
            "Installing collected packages: deepsnap\n",
            "Successfully installed deepsnap-0.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==2.0.0+cu118 --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install torch_geometric==2.3.1\n",
        "!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.0+cu118.html\n",
        "!pip install deepsnap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NOzHhv_K5R1C"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from deepsnap.batch import Batch\n",
        "from deepsnap.dataset import GraphDataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch_geometric.datasets import Planetoid\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Parameter\n",
        "from torch_geometric.nn.inits import glorot, zeros\n",
        "from torch_geometric.utils import add_remaining_self_loops\n",
        "from torch_scatter import scatter_add, scatter\n",
        "from torch.optim.optimizer import Optimizer\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JLLlaUp87n9A"
      },
      "outputs": [],
      "source": [
        "class GCNIDConvLayer(MessagePassing):\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 improved=False,\n",
        "                 cached=False,\n",
        "                 bias=True,\n",
        "                 normalize=True,\n",
        "                 **kwargs):\n",
        "        super(GCNIDConvLayer, self).__init__(aggr='add', **kwargs)\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.improved = improved\n",
        "        self.cached = cached\n",
        "        self.normalize = normalize\n",
        "\n",
        "        self.weight = Parameter(torch.Tensor(in_channels, out_channels))\n",
        "        self.weight_id = Parameter(torch.Tensor(in_channels, out_channels))\n",
        "\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.Tensor(out_channels))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        glorot(self.weight)\n",
        "        glorot(self.weight_id)\n",
        "        zeros(self.bias)\n",
        "        self.cached_result = None\n",
        "        self.cached_num_edges = None\n",
        "\n",
        "    @staticmethod\n",
        "    def norm(edge_index,\n",
        "             num_nodes,\n",
        "             edge_weight=None,\n",
        "             improved=False,\n",
        "             dtype=None):\n",
        "        if edge_weight is None:\n",
        "            edge_weight = torch.ones((edge_index.size(1), ),\n",
        "                                     dtype=dtype,\n",
        "                                     device=edge_index.device)\n",
        "\n",
        "        fill_value = 1.0 if not improved else 2.0\n",
        "        edge_index, edge_weight = add_remaining_self_loops(\n",
        "            edge_index, edge_weight, fill_value, num_nodes)\n",
        "\n",
        "        row, col = edge_index\n",
        "        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
        "        deg_inv_sqrt = deg.pow(-0.5)\n",
        "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
        "\n",
        "        return edge_index, deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
        "\n",
        "    def forward(self, x, edge_index, id, edge_weight=None):\n",
        "        x_id = torch.index_select(x, dim=0, index=id)\n",
        "        x_id = torch.matmul(x_id, self.weight_id)\n",
        "        x = torch.matmul(x, self.weight)\n",
        "        x.index_add_(0, id, x_id)\n",
        "\n",
        "        if self.cached and self.cached_result is not None:\n",
        "            if edge_index.size(1) != self.cached_num_edges:\n",
        "                raise RuntimeError(\n",
        "                    'Cached {} number of edges, but found {}. Please '\n",
        "                    'disable the caching behavior of this layer by removing '\n",
        "                    'the `cached=True` argument in its constructor.'.format(\n",
        "                        self.cached_num_edges, edge_index.size(1)))\n",
        "\n",
        "        if not self.cached or self.cached_result is None:\n",
        "            self.cached_num_edges = edge_index.size(1)\n",
        "            if self.normalize:\n",
        "                edge_index, norm = self.norm(edge_index, x.size(self.node_dim),\n",
        "                                             edge_weight, self.improved,\n",
        "                                             x.dtype)\n",
        "            else:\n",
        "                norm = edge_weight\n",
        "            self.cached_result = edge_index, norm\n",
        "\n",
        "\n",
        "        edge_index, norm = self.cached_result\n",
        "\n",
        "        return self.propagate(edge_index, x=x, norm=norm)\n",
        "\n",
        "    def message(self, x_j, norm):\n",
        "        return norm.view(-1, 1) * x_j if norm is not None else x_j\n",
        "\n",
        "    def update(self, aggr_out):\n",
        "        if self.bias is not None:\n",
        "            aggr_out = aggr_out + self.bias\n",
        "        return aggr_out\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels,\n",
        "                                   self.out_channels)\n",
        "class CRD(nn.Module):\n",
        "    def __init__(self, dim_in, dim_out, bias=False, p=0.6, **kwargs):\n",
        "        super(CRD, self).__init__()\n",
        "        self.conv = GCNIDConvLayer(dim_in, dim_out, bias=bias)\n",
        "        self.p = p\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.conv.reset_parameters()\n",
        "\n",
        "    def forward(self, batch):\n",
        "        batch.node_feature = F.relu(self.conv(batch.node_feature, batch.edge_index,\n",
        "                                        batch.node_id_index)) # prelu, swish, leaky relu\n",
        "        batch.node_feature = F.dropout(batch.node_feature, p=self.p, training=self.training)\n",
        "        return batch\n",
        "\n",
        "class CLS(nn.Module):\n",
        "    def __init__(self, dim_in, dim_out, bias=False, **kwargs):\n",
        "        super(CLS, self).__init__()\n",
        "        self.conv = GCNIDConvLayer(dim_in, dim_out, bias=bias)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.conv.reset_parameters()\n",
        "\n",
        "    def forward(self, batch):\n",
        "        batch.node_feature = self.conv(batch.node_feature, batch.edge_index,\n",
        "                                        batch.node_id_index)\n",
        "        batch.node_feature = F.log_softmax(batch.node_feature, dim=1)\n",
        "        return batch\n",
        "\n",
        "class GCNIDConv(nn.Module):\n",
        "    def __init__(self, dim_in, dim_out, bias=False, p=0.6, **kwargs):\n",
        "        super(GCNIDConv, self).__init__()\n",
        "\n",
        "        self.p = p\n",
        "\n",
        "        self.crd = CRD(dim_in, 16, bias=bias, p=p) # 0.0, 0.3, 0.6\n",
        "        self.cls = CLS(16, dim_out, bias=bias)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.crd.reset_parameters()\n",
        "        self.cls.reset_parameters()\n",
        "\n",
        "    def forward(self, batch):\n",
        "        batch = self.crd(batch)\n",
        "        batch = self.cls(batch)\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "khjwDE3z782I"
      },
      "outputs": [],
      "source": [
        "def ego_nets(graph, radius=2):\n",
        "    # get networks for mini batch node/graph prediction tasks\n",
        "    # color center\n",
        "    egos = []\n",
        "    n = graph.num_nodes\n",
        "    # A proper deepsnap.G should have nodes indexed from 0 to n-1\n",
        "    for i in range(n):\n",
        "        if radius > 4:\n",
        "            egos.append(graph.G)\n",
        "        else:\n",
        "            egos.append(nx.ego_graph(graph.G, i, radius=radius))\n",
        "    # relabel egos: keep center node ID, relabel other node IDs\n",
        "    G = graph.G.__class__()\n",
        "    id_bias = n\n",
        "    for i in range(len(egos)):\n",
        "        G.add_node(i, **egos[i].nodes(data=True)[i])\n",
        "    for i in range(len(egos)):\n",
        "        keys = list(egos[i].nodes)\n",
        "        keys.remove(i)\n",
        "        id_cur = egos[i].number_of_nodes() - 1\n",
        "        vals = range(id_bias, id_bias + id_cur)\n",
        "        id_bias += id_cur\n",
        "        mapping = dict(zip(keys, vals))\n",
        "        ego = nx.relabel_nodes(egos[i], mapping, copy=True)\n",
        "        G.add_nodes_from(ego.nodes(data=True))\n",
        "        G.add_edges_from(ego.edges(data=True))\n",
        "    graph.G = G\n",
        "    graph.node_id_index = torch.arange(len(egos))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKdYdfyE8EAl",
        "outputId": "c7fa7a5f-be5f-4c4d-c6c3-92c66e594b6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "# Load the Cora dataset and create a DataLoader object\n",
        "dataset_raw = Planetoid(root='/tmp/Cora', name='Cora')\n",
        "# dataset_raw = Planetoid(root='/tmp/CiteSeer', name='CiteSeer')\n",
        "# dataset_raw = Planetoid(root='/tmp/PubMed', name='PubMed')\n",
        "\n",
        "graphs = GraphDataset.pyg_to_graphs(dataset_raw)\n",
        "\n",
        "dataset = GraphDataset(graphs, task='node')\n",
        "\n",
        "datasets = dataset.split(transductive=True,\n",
        "                                 split_ratio=[0.7, 0.15, 0.15], # 0.8, 0.1, 0.1\n",
        "                                 shuffle=True)\n",
        "\n",
        "def transform_after_split(datasets):\n",
        "\n",
        "    # Dataset transformation after train/val/test split\n",
        "    # :param dataset: A list of DeepSNAP dataset objects\n",
        "    # :return: A list of transformed DeepSNAP dataset objects\n",
        "\n",
        "    for split_dataset in datasets:\n",
        "            split_dataset.apply_transform(ego_nets,\n",
        "                                          radius=2,\n",
        "                                          update_tensor=True,\n",
        "                                          update_graph=False)\n",
        "            split_dataset.task = 'node'\n",
        "    return datasets\n",
        "\n",
        "datasets = transform_after_split(datasets)\n",
        "\n",
        "def set_dataset_info(datasets):\n",
        "    r\"\"\"\n",
        "    Set global dataset information\n",
        "\n",
        "    Args:\n",
        "        datasets: List of dataset object\n",
        "\n",
        "    \"\"\"\n",
        "    # get dim_in and dim_out\n",
        "    try:\n",
        "        dim_in = datasets[0].num_node_features\n",
        "    except Exception:\n",
        "        dim_in = 1\n",
        "    try:\n",
        "        dim_out = datasets[0].num_labels\n",
        "        if 'classification' in 'classification_multi' and \\\n",
        "                dim_out == 2:\n",
        "            dim_out = 1\n",
        "    except Exception:\n",
        "        dim_out = 1\n",
        "\n",
        "    # count number of dataset splits\n",
        "    num_splits = len(datasets)\n",
        "\n",
        "    return dim_in, dim_out, num_splits\n",
        "\n",
        "dim_in, dim_out, num_splits = set_dataset_info(datasets)\n",
        "\n",
        "def create_loader(datasets):\n",
        "    loader_train = DataLoader(datasets[0],\n",
        "                              collate_fn=Batch.collate(),\n",
        "                              batch_size=32, # 16, 64\n",
        "                              shuffle=True,\n",
        "                              num_workers=0,\n",
        "                              pin_memory=False)\n",
        "\n",
        "    loaders = [loader_train]\n",
        "    for i in range(1, len(datasets)):\n",
        "        loaders.append(\n",
        "            DataLoader(datasets[i],\n",
        "                       collate_fn=Batch.collate(),\n",
        "                       batch_size=32, # 16, 64\n",
        "                       shuffle=False,\n",
        "                       num_workers=0,\n",
        "                       pin_memory=False))\n",
        "\n",
        "    return loaders\n",
        "\n",
        "loaders = create_loader(datasets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_3hRUig9-kid"
      },
      "outputs": [],
      "source": [
        "# Define the device to use for computation (CPU or GPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "# Create an instance of the model and move it to the device\n",
        "model = GCNIDConv(dim_in, dim_out).to(device)\n",
        "model.reset_parameters()\n",
        "\n",
        "# Define the loss function as cross entropy and move it to the device\n",
        "# criterion = nn.CrossEntropyLoss().to(device)\n",
        "criterion = F.nll_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7tB3pAWDCpnL"
      },
      "outputs": [],
      "source": [
        "class KFAC(Optimizer):\n",
        "\n",
        "    def __init__(self, net, eps, sua=False, pi=False, update_freq=1,\n",
        "                 alpha=1.0, constraint_norm=False):\n",
        "        \"\"\" K-FAC Preconditionner for Linear and Conv2d layers.\n",
        "        Computes the K-FAC of the second moment of the gradients.\n",
        "        It works for Linear and Conv2d layers and silently skip other layers.\n",
        "        Args:\n",
        "            net (torch.nn.Module): Network to precondition.\n",
        "            eps (float): Tikhonov regularization parameter for the inverses.\n",
        "            sua (bool): Applies SUA approximation.\n",
        "            pi (bool): Computes pi correction for Tikhonov regularization.\n",
        "            update_freq (int): Perform inverses every update_freq updates.\n",
        "            alpha (float): Running average parameter (if == 1, no r. ave.).\n",
        "            constraint_norm (bool): Scale the gradients by the squared\n",
        "                fisher norm.\n",
        "        \"\"\"\n",
        "        self.eps = eps\n",
        "        self.sua = sua\n",
        "        self.pi = pi\n",
        "        self.update_freq = update_freq\n",
        "        self.alpha = alpha\n",
        "        self.constraint_norm = constraint_norm\n",
        "        self.params = []\n",
        "        self._fwd_handles = []\n",
        "        self._bwd_handles = []\n",
        "        self._iteration_counter = 0\n",
        "\n",
        "        for mod in net.modules():\n",
        "            mod_name = mod.__class__.__name__\n",
        "            if mod_name in ['CRD', 'CLS']:\n",
        "                handle = mod.register_forward_pre_hook(self._save_input)\n",
        "                self._fwd_handles.append(handle)\n",
        "\n",
        "                for sub_mod in mod.modules():\n",
        "                    i_sub_mod = 0\n",
        "                    if hasattr(sub_mod, 'weight'):\n",
        "                        assert i_sub_mod == 0\n",
        "                        # handle = sub_mod.register_backward_hook(self._save_grad_output)\n",
        "                        handle = sub_mod.register_full_backward_hook(self._save_grad_output)\n",
        "                        self._bwd_handles.append(handle)\n",
        "\n",
        "                        params = [sub_mod.weight]\n",
        "                        if sub_mod.bias is not None:\n",
        "                            params.append(sub_mod.bias)\n",
        "\n",
        "                        d = {'params': params, 'mod': mod, 'sub_mod': sub_mod}\n",
        "                        self.params.append(d)\n",
        "                        i_sub_mod += 1\n",
        "\n",
        "        super(KFAC, self).__init__(self.params, {})\n",
        "\n",
        "    def step(self, update_stats=True, update_params=True, lam=0.):\n",
        "        \"\"\"Performs one step of preconditioning.\"\"\"\n",
        "        self.lam = lam\n",
        "        fisher_norm = 0.\n",
        "        for group in self.param_groups:\n",
        "\n",
        "            if len(group['params']) == 2:\n",
        "                weight, bias = group['params']\n",
        "            else:\n",
        "                weight = group['params'][0]\n",
        "                bias = None\n",
        "            state = self.state[weight]\n",
        "\n",
        "            # Update convariances and inverses\n",
        "            if update_stats:\n",
        "                if self._iteration_counter % self.update_freq == 0:\n",
        "                    self._compute_covs(group, state)\n",
        "                    ixxt, iggt = self._inv_covs(state['xxt'], state['ggt'],\n",
        "                                                state['num_locations'])\n",
        "                    state['ixxt'] = ixxt\n",
        "                    state['iggt'] = iggt\n",
        "                else:\n",
        "                    if self.alpha != 1:\n",
        "                        self._compute_covs(group, state)\n",
        "\n",
        "            if update_params:\n",
        "                gw, gb = self._precond(weight, bias, group, state)\n",
        "\n",
        "                # Updating gradients\n",
        "                if self.constraint_norm:\n",
        "                    fisher_norm += (weight.grad * gw).sum()\n",
        "\n",
        "                weight.grad.data = gw\n",
        "                if bias is not None:\n",
        "                    if self.constraint_norm:\n",
        "                        fisher_norm += (bias.grad * gb).sum()\n",
        "                    bias.grad.data = gb\n",
        "\n",
        "            # Cleaning\n",
        "            if 'x' in self.state[group['mod']]:\n",
        "                del self.state[group['mod']]['x']\n",
        "            if 'gy' in self.state[group['mod']]:\n",
        "                del self.state[group['mod']]['gy']\n",
        "\n",
        "        # Eventually scale the norm of the gradients\n",
        "        if update_params and self.constraint_norm:\n",
        "            scale = (1. / fisher_norm) ** 0.5\n",
        "            for group in self.param_groups:\n",
        "                for param in group['params']:\n",
        "                    print(param.shape, param)\n",
        "                    param.grad.data *= scale\n",
        "\n",
        "        if update_stats:\n",
        "            self._iteration_counter += 1\n",
        "\n",
        "    def _save_input(self, mod, i):\n",
        "        \"\"\"Saves input of layer to compute covariance.\"\"\"\n",
        "        # i = (x, edge_index)\n",
        "        if mod.training:\n",
        "            self.state[mod]['x'] = i[0]['node_feature'] #\n",
        "\n",
        "            # self.mask = i[-1]\n",
        "\n",
        "    def _save_grad_output(self, mod, grad_input, grad_output):\n",
        "        \"\"\"Saves grad on output of layer to compute covariance.\"\"\"\n",
        "        if mod.training:\n",
        "\n",
        "            self.state[mod]['gy'] = grad_output[0] * grad_output[0].size(1)\n",
        "\n",
        "            self.cached_result = mod.cached_result\n",
        "\n",
        "\n",
        "\n",
        "    def _precond(self, weight, bias, group, state):\n",
        "        \"\"\"Applies preconditioning.\"\"\"\n",
        "        ixxt = state['ixxt'] # [d_in x d_in]\n",
        "        iggt = state['iggt'] # [d_out x d_out]\n",
        "        g = weight.grad.data # [d_in x d_out]\n",
        "        s = g.shape\n",
        "\n",
        "        g = g.contiguous().view(-1, g.shape[-1])\n",
        "\n",
        "        if bias is not None:\n",
        "            gb = bias.grad.data\n",
        "            g = torch.cat([g, gb.view(1, gb.shape[0])], dim=0)\n",
        "\n",
        "        g = torch.mm(ixxt, torch.mm(g, iggt))\n",
        "\n",
        "        if bias is not None:\n",
        "            gb = g[-1].contiguous().view(*bias.shape)\n",
        "            g = g[:-1]\n",
        "        else:\n",
        "            gb = None\n",
        "        g = g.contiguous().view(*s)\n",
        "        return g, gb\n",
        "\n",
        "    def _compute_covs(self, group, state):\n",
        "        \"\"\"Computes the covariances.\"\"\"\n",
        "        sub_mod = group['sub_mod']\n",
        "        x = self.state[group['mod']]['x'] # [n x d_in] #\n",
        "        gy = self.state[group['sub_mod']]['gy'] # [n x d_out] #\n",
        "\n",
        "        edge_index, edge_weight = self.cached_result # [2, n_edges], [n_edges] #\n",
        "\n",
        "        # n = float(self.mask.sum() + self.lam*((~self.mask).sum()))\n",
        "        n = float(x.shape[0])\n",
        "\n",
        "        x = scatter(x[edge_index[0]]*edge_weight[:, None], edge_index[1], dim=0)\n",
        "\n",
        "        x = x.data.t()\n",
        "\n",
        "        if sub_mod.weight.ndim == 3:\n",
        "            x = x.repeat(sub_mod.weight.shape[0], 1)\n",
        "\n",
        "\n",
        "\n",
        "        if sub_mod.bias is not None:\n",
        "            ones = torch.ones_like(x[:1])\n",
        "            x = torch.cat([x, ones], dim=0)\n",
        "\n",
        "        if self._iteration_counter == 0:\n",
        "            state['xxt'] = torch.mm(x, x.t()) / n\n",
        "        else:\n",
        "            state['xxt'].addmm_(mat1=x, mat2=x.t(),\n",
        "                                beta=(1. - self.alpha),\n",
        "                                alpha=self.alpha / n) #\n",
        "\n",
        "        gy = gy.data.t() # [d_out x n]\n",
        "\n",
        "        state['num_locations'] = 1\n",
        "        if self._iteration_counter == 0:\n",
        "            state['ggt'] = torch.mm(gy, gy.t()) / n\n",
        "        else:\n",
        "            state['ggt'].addmm_(mat1=gy, mat2=gy.t(),\n",
        "                                beta=(1. - self.alpha),\n",
        "                                alpha=self.alpha / n)\n",
        "\n",
        "    def _inv_covs(self, xxt, ggt, num_locations):\n",
        "        \"\"\"Inverses the covariances.\"\"\"\n",
        "        # Computes pi\n",
        "        pi = 1.0\n",
        "        if self.pi:\n",
        "            tx = torch.trace(xxt) * ggt.shape[0]\n",
        "            tg = torch.trace(ggt) * xxt.shape[0]\n",
        "            pi = (tx / tg)\n",
        "        # Regularizes and inverse\n",
        "        eps = self.eps / num_locations\n",
        "        diag_xxt = xxt.new(xxt.shape[0]).fill_((eps * pi) ** 0.5)\n",
        "        diag_ggt = ggt.new(ggt.shape[0]).fill_((eps / pi) ** 0.5)\n",
        "        ixxt = (xxt + torch.diag(diag_xxt)).inverse()\n",
        "        iggt = (ggt + torch.diag(diag_ggt)).inverse()\n",
        "\n",
        "        return ixxt, iggt\n",
        "\n",
        "    def __del__(self):\n",
        "        for handle in self._fwd_handles + self._bwd_handles:\n",
        "            handle.remove()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZG7XcK46tq_3"
      },
      "outputs": [],
      "source": [
        "preconditioner = KFAC(\n",
        "    model,  # Pass the model instance\n",
        "    eps=0.01,  # Set the epsilon value\n",
        "    sua=False,\n",
        "    pi=False,\n",
        "    update_freq=25,\n",
        "    alpha=1.0,\n",
        "    constraint_norm=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uWZscQxQeELq"
      },
      "outputs": [],
      "source": [
        "# Define the optimizer as Adam with learning rate and weight decay\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4) # 0.001, 0.1 # 0.0005\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) # 0.001, 0.1\n",
        "# Define a learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCU2-Pi8_Kd-",
        "outputId": "b2048bfe-929c-4f37-a11c-a1ef95f22e1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Train Loss: 1.9501, Train Acc: 0.1383, Val Loss: 1.8158, Val Acc: 0.4163, Test Loss: 1.8226, Test Acc: 0.4251\n",
            "Epoch: 2, Train Loss: 1.8050, Train Acc: 0.4174, Val Loss: 1.7062, Val Acc: 0.4803, Test Loss: 1.7157, Test Acc: 0.4865\n",
            "Epoch: 3, Train Loss: 1.6978, Train Acc: 0.4628, Val Loss: 1.5968, Val Acc: 0.5246, Test Loss: 1.6070, Test Acc: 0.5356\n",
            "Epoch: 4, Train Loss: 1.5870, Train Acc: 0.5003, Val Loss: 1.4901, Val Acc: 0.5764, Test Loss: 1.4990, Test Acc: 0.5774\n",
            "Epoch: 5, Train Loss: 1.4723, Train Acc: 0.5462, Val Loss: 1.3855, Val Acc: 0.6108, Test Loss: 1.3925, Test Acc: 0.6388\n",
            "Epoch: 6, Train Loss: 1.3615, Train Acc: 0.5799, Val Loss: 1.2825, Val Acc: 0.6773, Test Loss: 1.2881, Test Acc: 0.6781\n",
            "Epoch: 7, Train Loss: 1.2504, Train Acc: 0.6417, Val Loss: 1.1818, Val Acc: 0.7463, Test Loss: 1.1870, Test Acc: 0.7346\n",
            "Epoch: 8, Train Loss: 1.1587, Train Acc: 0.6770, Val Loss: 1.0856, Val Acc: 0.8054, Test Loss: 1.0899, Test Acc: 0.7887\n",
            "Epoch: 9, Train Loss: 1.0677, Train Acc: 0.7156, Val Loss: 0.9962, Val Acc: 0.8227, Test Loss: 0.9989, Test Acc: 0.8280\n",
            "Epoch: 10, Train Loss: 0.9707, Train Acc: 0.7551, Val Loss: 0.9138, Val Acc: 0.8350, Test Loss: 0.9148, Test Acc: 0.8452\n",
            "Epoch: 11, Train Loss: 0.9201, Train Acc: 0.7546, Val Loss: 0.8397, Val Acc: 0.8547, Test Loss: 0.8384, Test Acc: 0.8624\n",
            "Epoch: 12, Train Loss: 0.8549, Train Acc: 0.7799, Val Loss: 0.7735, Val Acc: 0.8596, Test Loss: 0.7694, Test Acc: 0.8771\n",
            "Epoch: 13, Train Loss: 0.7863, Train Acc: 0.8011, Val Loss: 0.7147, Val Acc: 0.8793, Test Loss: 0.7077, Test Acc: 0.8919\n",
            "Epoch: 14, Train Loss: 0.7287, Train Acc: 0.8322, Val Loss: 0.6629, Val Acc: 0.8867, Test Loss: 0.6524, Test Acc: 0.8919\n",
            "Epoch: 15, Train Loss: 0.6792, Train Acc: 0.8470, Val Loss: 0.6177, Val Acc: 0.8892, Test Loss: 0.6035, Test Acc: 0.8943\n",
            "Epoch: 16, Train Loss: 0.6307, Train Acc: 0.8501, Val Loss: 0.5789, Val Acc: 0.8916, Test Loss: 0.5608, Test Acc: 0.8993\n",
            "Epoch: 17, Train Loss: 0.5907, Train Acc: 0.8686, Val Loss: 0.5456, Val Acc: 0.8916, Test Loss: 0.5237, Test Acc: 0.9042\n",
            "Epoch: 18, Train Loss: 0.5574, Train Acc: 0.8644, Val Loss: 0.5173, Val Acc: 0.8892, Test Loss: 0.4917, Test Acc: 0.9140\n",
            "Epoch: 19, Train Loss: 0.5349, Train Acc: 0.8686, Val Loss: 0.4930, Val Acc: 0.8867, Test Loss: 0.4641, Test Acc: 0.9140\n",
            "Epoch: 20, Train Loss: 0.4929, Train Acc: 0.8755, Val Loss: 0.4725, Val Acc: 0.8842, Test Loss: 0.4406, Test Acc: 0.9165\n",
            "Epoch: 21, Train Loss: 0.4721, Train Acc: 0.8839, Val Loss: 0.4548, Val Acc: 0.8892, Test Loss: 0.4205, Test Acc: 0.9165\n",
            "Epoch: 22, Train Loss: 0.4411, Train Acc: 0.8955, Val Loss: 0.4400, Val Acc: 0.8941, Test Loss: 0.4034, Test Acc: 0.9189\n",
            "Epoch: 23, Train Loss: 0.4153, Train Acc: 0.8982, Val Loss: 0.4274, Val Acc: 0.8941, Test Loss: 0.3890, Test Acc: 0.9165\n",
            "Epoch: 24, Train Loss: 0.4104, Train Acc: 0.8955, Val Loss: 0.4170, Val Acc: 0.8916, Test Loss: 0.3769, Test Acc: 0.9066\n",
            "Epoch: 25, Train Loss: 0.3947, Train Acc: 0.8892, Val Loss: 0.4084, Val Acc: 0.8966, Test Loss: 0.3668, Test Acc: 0.9042\n",
            "Epoch: 26, Train Loss: 0.3810, Train Acc: 0.8881, Val Loss: 0.4012, Val Acc: 0.8966, Test Loss: 0.3586, Test Acc: 0.9017\n",
            "Epoch: 27, Train Loss: 0.3557, Train Acc: 0.9029, Val Loss: 0.3955, Val Acc: 0.8941, Test Loss: 0.3517, Test Acc: 0.9017\n",
            "Epoch: 28, Train Loss: 0.3496, Train Acc: 0.9013, Val Loss: 0.3910, Val Acc: 0.8941, Test Loss: 0.3460, Test Acc: 0.9042\n",
            "Epoch: 29, Train Loss: 0.3473, Train Acc: 0.8997, Val Loss: 0.3872, Val Acc: 0.8941, Test Loss: 0.3411, Test Acc: 0.9042\n",
            "Epoch: 30, Train Loss: 0.3209, Train Acc: 0.9029, Val Loss: 0.3840, Val Acc: 0.8916, Test Loss: 0.3368, Test Acc: 0.9042\n",
            "Epoch: 31, Train Loss: 0.3220, Train Acc: 0.9092, Val Loss: 0.3815, Val Acc: 0.8916, Test Loss: 0.3330, Test Acc: 0.9066\n",
            "Epoch: 32, Train Loss: 0.2885, Train Acc: 0.9193, Val Loss: 0.3797, Val Acc: 0.8916, Test Loss: 0.3298, Test Acc: 0.9066\n",
            "Epoch: 33, Train Loss: 0.2756, Train Acc: 0.9293, Val Loss: 0.3784, Val Acc: 0.8916, Test Loss: 0.3269, Test Acc: 0.9115\n",
            "Epoch: 34, Train Loss: 0.2907, Train Acc: 0.9182, Val Loss: 0.3774, Val Acc: 0.8916, Test Loss: 0.3245, Test Acc: 0.9115\n",
            "Epoch: 35, Train Loss: 0.2771, Train Acc: 0.9251, Val Loss: 0.3767, Val Acc: 0.8916, Test Loss: 0.3223, Test Acc: 0.9115\n",
            "Epoch: 36, Train Loss: 0.2768, Train Acc: 0.9230, Val Loss: 0.3761, Val Acc: 0.8941, Test Loss: 0.3204, Test Acc: 0.9115\n",
            "Epoch: 37, Train Loss: 0.2589, Train Acc: 0.9256, Val Loss: 0.3756, Val Acc: 0.8941, Test Loss: 0.3186, Test Acc: 0.9115\n",
            "Epoch: 38, Train Loss: 0.2799, Train Acc: 0.9198, Val Loss: 0.3750, Val Acc: 0.8941, Test Loss: 0.3170, Test Acc: 0.9091\n",
            "Epoch: 39, Train Loss: 0.2632, Train Acc: 0.9266, Val Loss: 0.3746, Val Acc: 0.8966, Test Loss: 0.3158, Test Acc: 0.9091\n",
            "Epoch: 40, Train Loss: 0.2622, Train Acc: 0.9230, Val Loss: 0.3743, Val Acc: 0.8941, Test Loss: 0.3148, Test Acc: 0.9091\n",
            "Epoch: 41, Train Loss: 0.2552, Train Acc: 0.9282, Val Loss: 0.3742, Val Acc: 0.8916, Test Loss: 0.3141, Test Acc: 0.9066\n",
            "Epoch: 42, Train Loss: 0.2649, Train Acc: 0.9240, Val Loss: 0.3743, Val Acc: 0.8892, Test Loss: 0.3137, Test Acc: 0.9066\n",
            "Epoch: 43, Train Loss: 0.2567, Train Acc: 0.9293, Val Loss: 0.3744, Val Acc: 0.8916, Test Loss: 0.3133, Test Acc: 0.9115\n",
            "Epoch: 44, Train Loss: 0.2373, Train Acc: 0.9335, Val Loss: 0.3745, Val Acc: 0.8941, Test Loss: 0.3128, Test Acc: 0.9115\n",
            "Epoch: 45, Train Loss: 0.2461, Train Acc: 0.9256, Val Loss: 0.3746, Val Acc: 0.8941, Test Loss: 0.3123, Test Acc: 0.9091\n",
            "Epoch: 46, Train Loss: 0.2384, Train Acc: 0.9346, Val Loss: 0.3746, Val Acc: 0.8966, Test Loss: 0.3120, Test Acc: 0.9091\n",
            "Epoch: 47, Train Loss: 0.2398, Train Acc: 0.9266, Val Loss: 0.3746, Val Acc: 0.8966, Test Loss: 0.3118, Test Acc: 0.9091\n",
            "Epoch: 48, Train Loss: 0.2377, Train Acc: 0.9277, Val Loss: 0.3747, Val Acc: 0.8966, Test Loss: 0.3116, Test Acc: 0.9091\n",
            "Epoch: 49, Train Loss: 0.2365, Train Acc: 0.9351, Val Loss: 0.3748, Val Acc: 0.8966, Test Loss: 0.3117, Test Acc: 0.9091\n",
            "Epoch: 50, Train Loss: 0.2220, Train Acc: 0.9430, Val Loss: 0.3750, Val Acc: 0.8966, Test Loss: 0.3116, Test Acc: 0.9091\n",
            "Epoch: 51, Train Loss: 0.2298, Train Acc: 0.9319, Val Loss: 0.3752, Val Acc: 0.8966, Test Loss: 0.3115, Test Acc: 0.9066\n",
            "Epoch: 52, Train Loss: 0.2382, Train Acc: 0.9298, Val Loss: 0.3753, Val Acc: 0.8966, Test Loss: 0.3114, Test Acc: 0.9066\n",
            "Epoch: 53, Train Loss: 0.2302, Train Acc: 0.9398, Val Loss: 0.3755, Val Acc: 0.8966, Test Loss: 0.3113, Test Acc: 0.9066\n",
            "Epoch: 54, Train Loss: 0.2228, Train Acc: 0.9377, Val Loss: 0.3756, Val Acc: 0.8990, Test Loss: 0.3113, Test Acc: 0.9066\n",
            "Epoch: 55, Train Loss: 0.2072, Train Acc: 0.9456, Val Loss: 0.3758, Val Acc: 0.8990, Test Loss: 0.3113, Test Acc: 0.9066\n",
            "Epoch: 56, Train Loss: 0.2102, Train Acc: 0.9404, Val Loss: 0.3760, Val Acc: 0.9015, Test Loss: 0.3111, Test Acc: 0.9066\n",
            "Epoch: 57, Train Loss: 0.2194, Train Acc: 0.9372, Val Loss: 0.3761, Val Acc: 0.8990, Test Loss: 0.3110, Test Acc: 0.9091\n",
            "Epoch: 58, Train Loss: 0.2239, Train Acc: 0.9414, Val Loss: 0.3761, Val Acc: 0.8990, Test Loss: 0.3109, Test Acc: 0.9091\n",
            "Epoch: 59, Train Loss: 0.2154, Train Acc: 0.9404, Val Loss: 0.3761, Val Acc: 0.8966, Test Loss: 0.3108, Test Acc: 0.9091\n",
            "Epoch: 60, Train Loss: 0.2025, Train Acc: 0.9367, Val Loss: 0.3760, Val Acc: 0.8966, Test Loss: 0.3108, Test Acc: 0.9066\n",
            "Epoch: 61, Train Loss: 0.2106, Train Acc: 0.9377, Val Loss: 0.3761, Val Acc: 0.8966, Test Loss: 0.3109, Test Acc: 0.9042\n",
            "Epoch: 62, Train Loss: 0.2169, Train Acc: 0.9319, Val Loss: 0.3761, Val Acc: 0.8966, Test Loss: 0.3111, Test Acc: 0.9042\n",
            "Epoch: 63, Train Loss: 0.2003, Train Acc: 0.9478, Val Loss: 0.3762, Val Acc: 0.8990, Test Loss: 0.3112, Test Acc: 0.9017\n",
            "Epoch: 64, Train Loss: 0.2147, Train Acc: 0.9383, Val Loss: 0.3763, Val Acc: 0.8990, Test Loss: 0.3114, Test Acc: 0.9017\n",
            "Epoch: 65, Train Loss: 0.2118, Train Acc: 0.9435, Val Loss: 0.3765, Val Acc: 0.8990, Test Loss: 0.3115, Test Acc: 0.9017\n",
            "Epoch: 66, Train Loss: 0.2149, Train Acc: 0.9372, Val Loss: 0.3766, Val Acc: 0.8990, Test Loss: 0.3116, Test Acc: 0.9017\n",
            "Epoch: 67, Train Loss: 0.2083, Train Acc: 0.9393, Val Loss: 0.3767, Val Acc: 0.8990, Test Loss: 0.3118, Test Acc: 0.9017\n",
            "Epoch: 68, Train Loss: 0.2036, Train Acc: 0.9446, Val Loss: 0.3768, Val Acc: 0.8990, Test Loss: 0.3118, Test Acc: 0.9017\n",
            "Epoch: 69, Train Loss: 0.2161, Train Acc: 0.9361, Val Loss: 0.3770, Val Acc: 0.8990, Test Loss: 0.3119, Test Acc: 0.9017\n",
            "Epoch: 70, Train Loss: 0.2085, Train Acc: 0.9377, Val Loss: 0.3771, Val Acc: 0.8990, Test Loss: 0.3119, Test Acc: 0.9017\n",
            "Epoch: 71, Train Loss: 0.2096, Train Acc: 0.9409, Val Loss: 0.3773, Val Acc: 0.8990, Test Loss: 0.3119, Test Acc: 0.9017\n",
            "Epoch: 72, Train Loss: 0.1933, Train Acc: 0.9478, Val Loss: 0.3775, Val Acc: 0.8990, Test Loss: 0.3119, Test Acc: 0.9017\n",
            "Epoch: 73, Train Loss: 0.2004, Train Acc: 0.9446, Val Loss: 0.3777, Val Acc: 0.8990, Test Loss: 0.3119, Test Acc: 0.9017\n",
            "Epoch: 74, Train Loss: 0.2022, Train Acc: 0.9435, Val Loss: 0.3778, Val Acc: 0.8990, Test Loss: 0.3119, Test Acc: 0.9017\n",
            "Epoch: 75, Train Loss: 0.1888, Train Acc: 0.9551, Val Loss: 0.3780, Val Acc: 0.8990, Test Loss: 0.3119, Test Acc: 0.9017\n",
            "Epoch: 76, Train Loss: 0.2077, Train Acc: 0.9351, Val Loss: 0.3781, Val Acc: 0.8990, Test Loss: 0.3120, Test Acc: 0.9017\n",
            "Epoch: 77, Train Loss: 0.2004, Train Acc: 0.9409, Val Loss: 0.3783, Val Acc: 0.8990, Test Loss: 0.3120, Test Acc: 0.9017\n",
            "Epoch: 78, Train Loss: 0.1992, Train Acc: 0.9425, Val Loss: 0.3784, Val Acc: 0.8990, Test Loss: 0.3121, Test Acc: 0.9017\n",
            "Epoch: 79, Train Loss: 0.2137, Train Acc: 0.9435, Val Loss: 0.3785, Val Acc: 0.8990, Test Loss: 0.3122, Test Acc: 0.9017\n",
            "Epoch: 80, Train Loss: 0.2060, Train Acc: 0.9446, Val Loss: 0.3786, Val Acc: 0.8990, Test Loss: 0.3123, Test Acc: 0.9017\n",
            "Epoch: 81, Train Loss: 0.2055, Train Acc: 0.9420, Val Loss: 0.3787, Val Acc: 0.8990, Test Loss: 0.3123, Test Acc: 0.9017\n",
            "Epoch: 82, Train Loss: 0.2127, Train Acc: 0.9393, Val Loss: 0.3787, Val Acc: 0.8990, Test Loss: 0.3124, Test Acc: 0.9017\n",
            "Epoch: 83, Train Loss: 0.1884, Train Acc: 0.9504, Val Loss: 0.3788, Val Acc: 0.8990, Test Loss: 0.3125, Test Acc: 0.9017\n",
            "Epoch: 84, Train Loss: 0.2118, Train Acc: 0.9398, Val Loss: 0.3788, Val Acc: 0.8990, Test Loss: 0.3125, Test Acc: 0.9017\n",
            "Epoch: 85, Train Loss: 0.2087, Train Acc: 0.9478, Val Loss: 0.3789, Val Acc: 0.8990, Test Loss: 0.3126, Test Acc: 0.9017\n",
            "Epoch: 86, Train Loss: 0.1994, Train Acc: 0.9456, Val Loss: 0.3789, Val Acc: 0.8990, Test Loss: 0.3127, Test Acc: 0.9017\n",
            "Epoch: 87, Train Loss: 0.2040, Train Acc: 0.9435, Val Loss: 0.3790, Val Acc: 0.8990, Test Loss: 0.3127, Test Acc: 0.9017\n",
            "Epoch: 88, Train Loss: 0.1897, Train Acc: 0.9509, Val Loss: 0.3790, Val Acc: 0.8990, Test Loss: 0.3127, Test Acc: 0.9017\n",
            "Epoch: 89, Train Loss: 0.2050, Train Acc: 0.9398, Val Loss: 0.3790, Val Acc: 0.8990, Test Loss: 0.3128, Test Acc: 0.9017\n",
            "Epoch: 90, Train Loss: 0.2104, Train Acc: 0.9398, Val Loss: 0.3791, Val Acc: 0.8990, Test Loss: 0.3128, Test Acc: 0.9017\n",
            "Epoch: 91, Train Loss: 0.2055, Train Acc: 0.9404, Val Loss: 0.3791, Val Acc: 0.8990, Test Loss: 0.3128, Test Acc: 0.9017\n",
            "Epoch: 92, Train Loss: 0.2021, Train Acc: 0.9388, Val Loss: 0.3791, Val Acc: 0.8990, Test Loss: 0.3128, Test Acc: 0.9017\n",
            "Epoch: 93, Train Loss: 0.1962, Train Acc: 0.9435, Val Loss: 0.3791, Val Acc: 0.8990, Test Loss: 0.3128, Test Acc: 0.9017\n",
            "Epoch: 94, Train Loss: 0.1929, Train Acc: 0.9493, Val Loss: 0.3791, Val Acc: 0.8966, Test Loss: 0.3128, Test Acc: 0.9017\n",
            "Epoch: 95, Train Loss: 0.1987, Train Acc: 0.9430, Val Loss: 0.3791, Val Acc: 0.8966, Test Loss: 0.3128, Test Acc: 0.9017\n",
            "Epoch: 96, Train Loss: 0.1967, Train Acc: 0.9483, Val Loss: 0.3791, Val Acc: 0.8966, Test Loss: 0.3128, Test Acc: 0.9017\n",
            "Epoch: 97, Train Loss: 0.1990, Train Acc: 0.9430, Val Loss: 0.3792, Val Acc: 0.8966, Test Loss: 0.3128, Test Acc: 0.9017\n",
            "Epoch: 98, Train Loss: 0.1976, Train Acc: 0.9472, Val Loss: 0.3792, Val Acc: 0.8966, Test Loss: 0.3128, Test Acc: 0.9017\n",
            "Epoch: 99, Train Loss: 0.2057, Train Acc: 0.9420, Val Loss: 0.3792, Val Acc: 0.8966, Test Loss: 0.3128, Test Acc: 0.9017\n",
            "Epoch: 100, Train Loss: 0.1985, Train Acc: 0.9451, Val Loss: 0.3792, Val Acc: 0.8966, Test Loss: 0.3128, Test Acc: 0.9017\n",
            "Total training time: 101.76 seconds\n"
          ]
        }
      ],
      "source": [
        "# Define a function to compute the accuracy of the model predictions\n",
        "def accuracy(output, labels):\n",
        "    preds = output.max(1)[1].type_as(labels)\n",
        "    correct = preds.eq(labels).double()\n",
        "    correct = correct.sum()\n",
        "    return correct / len(labels)\n",
        "\n",
        "# Define a function to train the model for one epoch\n",
        "def train(model, loader, optimizer, criterion, preconditioner=None, lam=0.):\n",
        "    model.train()\n",
        "    loss_all = 0\n",
        "    acc_all = 0\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        output = output.node_feature.index_select(0, data.node_label_index)\n",
        "        y = data.node_label.index_select(0, data.node_label_index)\n",
        "        y.requires_grad = False\n",
        "        loss = criterion(output, y)\n",
        "        loss.backward(retain_graph=True)\n",
        "\n",
        "        loss_all += data.num_graphs * loss.item()\n",
        "\n",
        "        acc_all += accuracy(output, y).item() * data.num_graphs\n",
        "        preconditioner.step(lam=lam)\n",
        "        optimizer.step()\n",
        "    return loss_all / len(loader.dataset), acc_all / len(loader.dataset)\n",
        "\n",
        "# Define a function to evaluate the model on validation or test set\n",
        "def eval(model, loader, criterion):\n",
        "    model.eval()\n",
        "    loss_all = 0\n",
        "    acc_all = 0\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data = data.to(device)\n",
        "            output = model(data)\n",
        "            output = output.node_feature.index_select(0, data.node_label_index)\n",
        "            y = data.node_label.index_select(0, data.node_label_index)\n",
        "            loss = criterion(output, y)\n",
        "            loss_all += data.num_graphs * loss.item()\n",
        "            acc_all += accuracy(output, y).item() * data.num_graphs\n",
        "    return loss_all / len(loader.dataset), acc_all / len(loader.dataset)\n",
        "\n",
        "# Define the number of epochs to train the model\n",
        "num_epochs = 100 # 200, 400\n",
        "\n",
        "# Create lists to store the training, validation and testing losses and accuracies\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "val_losses = []\n",
        "val_accs = []\n",
        "test_losses = []\n",
        "test_accs = []\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Train the model for num_epochs epochs and evaluate it after each epoch\n",
        "for epoch in range(num_epochs):\n",
        "    # lam = (float(epoch)/float(num_epochs))**0.1\n",
        "    train_loss, train_acc = train(model, loaders[0], optimizer, criterion, preconditioner, lam=0.) #\n",
        "    val_loss, val_acc = eval(model, loaders[1], criterion)\n",
        "    test_loss, test_acc = eval(model, loaders[2], criterion)\n",
        "    train_losses.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accs.append(val_acc)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accs.append(test_acc)\n",
        "    scheduler.step()\n",
        "    print(f'Epoch: {epoch + 1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
        "\n",
        "end_time = time.time()\n",
        "total_time = end_time - start_time\n",
        "print(f\"Total training time: {total_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training, validation, and test accuracy per epoch\n",
        "plt.plot(train_accs, label='Train Accuracy')\n",
        "plt.plot(val_accs, label='Validation Accuracy')\n",
        "plt.plot(test_accs, label='Test Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "# plt.title('Training, Validation, and Test Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "2uYbRA7c2tuH",
        "outputId": "ef7da18a-985a-4b8e-d187-ceeb09c2a758"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def params_count(model):\n",
        "    # Computes the number of parameters.\n",
        "\n",
        "    # Args:\n",
        "        # model (nn.Module): PyTorch model\n",
        "\n",
        "    return sum([p.numel() for p in model.parameters()])\n",
        "\n",
        "\n",
        "print(params_count(model))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfNywuTfhdNS",
        "outputId": "9a524b23-241e-4f02-a6eb-8ee234edc142"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "46080\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}